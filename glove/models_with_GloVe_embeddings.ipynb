{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "models_with_GloVe_embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cOr-RDGt6K_h",
        "5rByKgHl2SIa",
        "2r_QfCla2cBB"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zRBF2imvOk1",
        "colab_type": "code",
        "outputId": "1e2e8e4a-0202-4034-d18e-733238c0b438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msPHUR7yTSJx",
        "colab_type": "code",
        "outputId": "2e126a4a-f154-4667-b586-bef57e61d1d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        }
      },
      "source": [
        "!pip install ekphrasis"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ekphrasis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/37c59d65e78c3a2aaf662df58faca7250eb6b36c559b912a39a7ca204cfb/ekphrasis-0.5.1.tar.gz (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (4.28.1)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/a6/728666f39bfff1719fc94c481890b2106837da9318031f71a8424b662e12/colorama-0.4.1-py2.py3-none-any.whl\n",
            "Collecting ujson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.1.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.2.5)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ca/2d9a5030eaf1bcd925dab392762b9709a7ad4bd486a90599d93cd79cb188/ftfy-5.6.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.17.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->ekphrasis) (1.12.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->ekphrasis) (0.1.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis) (41.4.0)\n",
            "Building wheels for collected packages: ekphrasis, ujson, ftfy\n",
            "  Building wheel for ekphrasis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-cp36-none-any.whl size=82844 sha256=1400aaa24e86eb42e16a74cf3a9eae69b341346037164deb0a9dc82ed5506459\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/c5/9b/c9b60f535a2cf9fdbc92d84c4801a010c35a9cd348011ed2a1\n",
            "  Building wheel for ujson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ujson: filename=ujson-1.35-cp36-cp36m-linux_x86_64.whl size=68040 sha256=460d837956e8ae18cbdc886a83b9d9e9ccfc0bc64f8a2424abeaff396215ab7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/77/e4/0311145b9c2e2f01470e744855131f9e34d6919687550f87d1\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.6-cp36-none-any.whl size=44553 sha256=c4b30718ce8d3c6a9cef4504218624bd8292ff62ffe6b0b647c49f525d58065a\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/34/ce/cbb38d71543c408de56f3c5e26ce8ba495a0fa5a28eaaf1046\n",
            "Successfully built ekphrasis ujson ftfy\n",
            "Installing collected packages: colorama, ujson, ftfy, ekphrasis\n",
            "Successfully installed colorama-0.4.1 ekphrasis-0.5.1 ftfy-5.6 ujson-1.35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S91xgd19qQyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4cI9JrDpr8M",
        "colab_type": "code",
        "outputId": "f5d4dfb9-9cc6-4579-880b-2fe61f59d30f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import re\n",
        "import io\n",
        "import os\n",
        "import json\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from keras.models import load_model\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.layers import Input, Dense, Embedding, LSTM, Concatenate, Reshape, GRU, Bidirectional, Dropout, Conv1D, Flatten, MaxPool1D, TimeDistributed, Add\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import Callback\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVBtmndGwrbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd contextual_emotion_detection/glove"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAjZE0WbvN6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip \"../../drive/My Drive/glove.840B.300d.zip\" -d \"contextual_emotion_detection/data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xu5l_pLy9Ft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip \"../../drive/My Drive/embedding-results.zip\" -d \"contextual_emotion_detection/data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DQMcRFysgiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "projectpath = '..'\n",
        "datapath = os.path.join(projectpath, 'data')\n",
        "\n",
        "train_path = os.path.join(datapath, 'train.txt')\n",
        "test_path =  os.path.join(datapath, 'test.txt')\n",
        "validation_path = os.path.join(datapath, 'dev.txt')\n",
        "result_file_name = 'predictions.txt'\n",
        "glove_path = datapath\n",
        "sswe_path = os.path.join(datapath, 'embedding-results')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNJsUENAr4av",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCJDy6tzsFzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Callback function to compute F1 score for every epoch\n",
        "class ComputeMetricsCallback(Callback):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        x, y = self.test_data\n",
        "        r = self.model.predict(x)\n",
        "        compute_metrics(r, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOJXQaNx0TaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Configurations\n",
        "\n",
        "# 4 emotion classes: Happy, Sad, Angry, Others\n",
        "NUM_OF_CLASSES = 4\n",
        "\n",
        "# Maximum length of input phrase sequence\n",
        "MAX_SEQUENCE_LENGTH = 64\n",
        "\n",
        "# Maximum number of words to keep based on word frequency.\n",
        "MAX_NB_WORDS = 20000\n",
        "\n",
        "# The encoded vector dimension\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "# Batch size for training - helps prevent overfitting\n",
        "BATCH_SIZE = 200\n",
        "\n",
        "# LSTM layer size\n",
        "LSTM_DIM = 300\n",
        "\n",
        "# Lower learning rate helps the model to converge. But it\n",
        "# should be high enough that the model can converge in the given\n",
        "# number of iterations.\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Number of epochs in each iteration\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# The proportion of neural units that should be randomly dropped\n",
        "# for regularization technique helping to reduce overfitting \n",
        "DROPOUT_RATIO = 0.2\n",
        "\n",
        "label_to_emotion = {0: \"others\", 1: \"happy\", 2: \"sad\", 3: \"angry\"}\n",
        "emotion_to_label = {\"others\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3}\n",
        "angry_identifier = {\"others\": 0, \"happy\": 0, \"sad\": 0, \"angry\": 3}\n",
        "sad_identifier = {\"others\": 0, \"happy\": 0, \"sad\": 2, \"angry\": 0}\n",
        "happy_identifier = {\"others\": 0, \"happy\": 1, \"sad\": 0, \"angry\": 0}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04f4kcQ22ONQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_result_file(model, test_sequences, result_file_name = result_file_name):\n",
        "    test_data = pad_sequences(test_sequences, maxlen = MAX_SEQUENCE_LENGTH)\n",
        "    predictions = model.predict(test_data, batch_size = BATCH_SIZE)\n",
        "    predictions = predictions.argmax(axis = 1)\n",
        "\n",
        "    with io.open(result_file_name, \"w\", encoding = \"utf8\") as f:\n",
        "        f.write('\\t'.join([\"id\", \"turn1\", \"turn2\", \"turn3\", \"label\"]) + '\\n')\n",
        "        with io.open(test_path, encoding = \"utf8\") as fin:\n",
        "            fin.readline()\n",
        "            for line_number, line in enumerate(fin):\n",
        "                f.write('\\t'.join(line.strip().split('\\t')[:4]) + '\\t')\n",
        "                f.write(label_to_emotion[predictions[line_number]] + '\\n')\n",
        "    print(\"Model parameters: LSTM Dim : %d, Dropout : %.1f, Batch_size : %d, Learning rate : %.3f\"\n",
        "          % (LSTM_DIM, DROPOUT_RATIO, BATCH_SIZE, LEARNING_RATE))\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SebFE9U370Bq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the embedding matrix\n",
        "def create_embedding_matrix(word_index, embedding_file_path):\n",
        "    embeddings_vectors = {}\n",
        "    with io.open(embedding_file_path, encoding = \"utf8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split(' ')\n",
        "            word = values[0]\n",
        "            embedding_vector = np.array([float(val) for val in values[1:]])\n",
        "            embeddings_vectors[word] = embedding_vector\n",
        "    print('%s embedding vectors' % len(embeddings_vectors))\n",
        "\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_vectors.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # Assign the vector for words seen in the training dataset\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        else:\n",
        "            # Handle out of vocabulary words\n",
        "            out_of_vocabulary_word_vector = [np.random.normal(size = EMBEDDING_DIM)]\n",
        "            out_of_vocabulary_word_vector /= np.linalg.norm(out_of_vocabulary_word_vector)\n",
        "            embedding_matrix[i] = out_of_vocabulary_word_vector\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBsvKH4U-MAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute performance metrics like precision, recall and F1-score\n",
        "\n",
        "def compute_metrics(predictions, labels):\n",
        "    # Predictions to classes\n",
        "    class_predictions = to_categorical(predictions.argmax(axis = 1))\n",
        "    true_positives = np.sum(class_predictions * labels, axis = 0)\n",
        "    false_positives = np.sum(np.clip(class_predictions - labels, 0, 1), axis = 0)\n",
        "    false_negatives = np.sum(np.clip(labels - class_predictions, 0, 1), axis = 0)\n",
        "\n",
        "    print(\"True Positives per class : \", true_positives)\n",
        "    print(\"False Positives per class : \", false_positives)\n",
        "    print(\"False Negatives per class : \", false_negatives)\n",
        "\n",
        "    # *************************************** Macro metrics ***************************************\n",
        "    macro_precision = 0\n",
        "    macro_recall = 0\n",
        "    for category in range(1, NUM_OF_CLASSES):\n",
        "        precision = true_positives[category] / (true_positives[category] + false_positives[category])\n",
        "        macro_precision += precision\n",
        "        recall = true_positives[category] / (true_positives[category] + false_negatives[category])\n",
        "        macro_recall += recall\n",
        "        f1_score = (2 * recall * precision) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        print(\"Class %s : Precision : %.3f, Recall : %.3f, F1-Score : %.3f\" % (label_to_emotion[category], precision, recall, f1_score))\n",
        "\n",
        "    macro_precision /= 3\n",
        "    macro_recall /= 3\n",
        "    macro_f1_score = (2 * macro_recall * macro_precision ) / (macro_precision + macro_recall) if (macro_precision + macro_recall) > 0 else 0\n",
        "    print(\"Macro Precision : %.3f, Macro Recall : %.3f, Macro F1-Score : %.3f\" % (macro_precision, macro_recall, macro_f1_score))\n",
        "\n",
        "    # *************************************** Micro metrics ***************************************\n",
        "    true_positives = true_positives[1 : ].sum()\n",
        "    false_positives = false_positives[1 : ].sum()\n",
        "    false_negatives = false_negatives[1 : ].sum()\n",
        "\n",
        "    micro_precision = true_positives / (true_positives + false_positives)\n",
        "    micro_recall = true_positives / (true_positives + false_negatives)\n",
        "    micro_f1_score = (2 * micro_recall * micro_precision) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n",
        "\n",
        "    predictions = predictions.argmax(axis = 1)\n",
        "    labels = labels.argmax(axis = 1)\n",
        "    accuracy = np.mean(predictions == labels)\n",
        "    print(\"Accuracy : %.3f, Micro Precision : %.3f, Micro Recall : %.3f, Micro F1-Score : %.3f\" % (accuracy, micro_precision, micro_recall, micro_f1_score))\n",
        "\n",
        "    return {\n",
        "        \"macro\": [accuracy, macro_precision, macro_recall, macro_f1_score],\n",
        "        \"micro\": [accuracy, micro_precision, micro_recall, micro_f1_score]\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usWprzZL3U_M",
        "colab_type": "text"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BbBaTTY3XIi",
        "colab_type": "text"
      },
      "source": [
        "#### Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UQGP2s-3WDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bidirectional_lstm(embedding_matrix):\n",
        "    embedding_layer = Embedding(embedding_matrix.shape[0],\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights = [embedding_matrix],\n",
        "                                input_length = MAX_SEQUENCE_LENGTH,\n",
        "                                trainable = True)\n",
        "    bilstm = Sequential()\n",
        "    bilstm.add(embedding_layer)\n",
        "    bilstm.add(Dropout(DROPOUT_RATIO))\n",
        "    bilstm.add(Bidirectional(LSTM(LSTM_DIM)))\n",
        "    bilstm.add(Dropout(DROPOUT_RATIO))\n",
        "    bilstm.add(Dense(NUM_OF_CLASSES, activation = 'softmax'))\n",
        "    adam = optimizers.adam(lr = LEARNING_RATE)\n",
        "    bilstm.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = adam,\n",
        "                  metrics = ['acc'])\n",
        "    bilstm.summary()\n",
        "    return bilstm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzFu6i7J5SA8",
        "colab_type": "text"
      },
      "source": [
        "#### Gated Recurrent Unit Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp6oS7GZ5Uhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gated_recurrent_unit_network(embedding_matrix):\n",
        "    embedding_layer = Embedding(embedding_matrix.shape[0],\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights = [embedding_matrix],\n",
        "                                input_length = MAX_SEQUENCE_LENGTH,\n",
        "                                trainable = True)\n",
        "    gru = Sequential()\n",
        "    gru.add(embedding_layer)\n",
        "    gru.add(Dropout(DROPOUT_RATIO))\n",
        "    gru.add(GRU(128))\n",
        "    gru.add(Dropout(DROPOUT_RATIO))\n",
        "    gru.add(Dense(NUM_OF_CLASSES * 8, activation = 'relu'))\n",
        "    gru.add(Dropout(DROPOUT_RATIO))\n",
        "    gru.add(Dense(NUM_OF_CLASSES, activation = 'softmax'))\n",
        "    adam = optimizers.adam(lr = LEARNING_RATE)\n",
        "    gru.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = adam,\n",
        "                  metrics = ['acc'])\n",
        "    gru.summary()\n",
        "    return gru"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1j4z2-758a8",
        "colab_type": "text"
      },
      "source": [
        "#### Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FEf7lP36Bsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convolutional_neural_network(embedding_matrix):\n",
        "    embedding_layer = Embedding(embedding_matrix.shape[0],\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights = [embedding_matrix],\n",
        "                                input_length = MAX_SEQUENCE_LENGTH,\n",
        "                                trainable = True)\n",
        "    cnn = Sequential()\n",
        "    cnn.add(embedding_layer)\n",
        "    cnn.add(Dropout(DROPOUT_RATIO))\n",
        "    cnn.add(Conv1D(64, 3, padding = 'same'))\n",
        "    cnn.add(Conv1D(32, 3, padding = 'same'))\n",
        "    cnn.add(Conv1D(16, 3, padding = 'same'))\n",
        "    cnn.add(Flatten())\n",
        "    cnn.add(Dropout(DROPOUT_RATIO))\n",
        "    cnn.add(Dense(180, activation = 'relu'))\n",
        "    cnn.add(Dropout(DROPOUT_RATIO))\n",
        "    cnn.add(Dense(NUM_OF_CLASSES, activation = 'softmax'))\n",
        "    adam = optimizers.adam(lr = LEARNING_RATE)\n",
        "    cnn.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = adam,\n",
        "                  metrics = ['acc'])\n",
        "    cnn.summary()\n",
        "    return cnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oWqmVln6sTE",
        "colab_type": "text"
      },
      "source": [
        "#### CNN-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnHRZJlu6t-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CNN-LSTM Model\n",
        "def cnn_lstm(embedding_matrix):\n",
        "    embedding_layer = Embedding(embedding_matrix.shape[0],\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights = [embedding_matrix],\n",
        "                                input_length = MAX_SEQUENCE_LENGTH,\n",
        "                                trainable = True)\n",
        "    cnn_lstm = Sequential()\n",
        "    cnn_lstm.add(embedding_layer)\n",
        "    cnn_lstm.add(Conv1D(32, 3, padding = 'same', activation = 'relu'))\n",
        "    cnn_lstm.add(MaxPool1D(2))\n",
        "    cnn_lstm.add(LSTM(LSTM_DIM))\n",
        "    cnn_lstm.add(Dense(NUM_OF_CLASSES, activation = 'softmax'))\n",
        "    adam = optimizers.adam(lr = LEARNING_RATE)\n",
        "    cnn_lstm.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = adam,\n",
        "                  metrics = ['acc'])\n",
        "    cnn_lstm.summary()\n",
        "    return cnn_lstm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zYRLzP87RD7",
        "colab_type": "text"
      },
      "source": [
        "#### LSTM-CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFdD39oW7TB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_cnn(embedding_matrix):\n",
        "    embedding_layer = Embedding(embedding_matrix.shape[0],\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights = [embedding_matrix],\n",
        "                                input_length = MAX_SEQUENCE_LENGTH,\n",
        "                                trainable = True)\n",
        "    lstm_cnn = Sequential()\n",
        "    lstm_cnn.add(embedding_layer)\n",
        "    lstm_cnn.add(LSTM(LSTM_DIM, return_sequences = True))\n",
        "    lstm_cnn.add(Conv1D(32, 3, padding = 'same', activation = 'relu'))\n",
        "    lstm_cnn.add(MaxPool1D(2))\n",
        "    lstm_cnn.add(Flatten())\n",
        "    lstm_cnn.add(Dropout(DROPOUT_RATIO))\n",
        "    lstm_cnn.add(Dense(NUM_OF_CLASSES, activation = 'softmax'))\n",
        "    adam = optimizers.adam(lr = LEARNING_RATE)\n",
        "    lstm_cnn.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = adam,\n",
        "                  metrics = ['acc'])\n",
        "    lstm_cnn.summary()\n",
        "    return lstm_cnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI9hyx8s8AB0",
        "colab_type": "text"
      },
      "source": [
        "#### CNN-BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcCLy6bi78Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cnn_bilstm(embedding_matrix):\n",
        "    embedding_layer = Embedding(embedding_matrix.shape[0],\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights = [embedding_matrix],\n",
        "                                input_length = MAX_SEQUENCE_LENGTH,\n",
        "                                trainable = True)\n",
        "    cnn_bilstm = Sequential()\n",
        "    cnn_bilstm.add(embedding_layer)\n",
        "    cnn_bilstm.add(Conv1D(32, 3, padding = 'same', activation = 'relu'))\n",
        "    cnn_bilstm.add(MaxPool1D(2))\n",
        "    cnn_bilstm.add(Dropout(DROPOUT_RATIO))\n",
        "    cnn_bilstm.add(Bidirectional(LSTM(LSTM_DIM)))\n",
        "    cnn_bilstm.add(Dropout(DROPOUT_RATIO))\n",
        "    cnn_bilstm.add(Dense(NUM_OF_CLASSES, activation = 'softmax'))\n",
        "    adam = optimizers.adam(lr = LEARNING_RATE)\n",
        "    cnn_bilstm.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = adam,\n",
        "                  metrics = ['acc'])\n",
        "    cnn_bilstm.summary()\n",
        "    return cnn_bilstm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQQlxpoq8ebH",
        "colab_type": "text"
      },
      "source": [
        "#### BiLSTM-CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWaqQCq68f_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bilstm_cnn(embedding_matrix):\n",
        "    embedding_layer = Embedding(embedding_matrix.shape[0],\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights = [embedding_matrix],\n",
        "                                input_length = MAX_SEQUENCE_LENGTH,\n",
        "                                trainable = True)\n",
        "    bilstm_cnn = Sequential()\n",
        "    bilstm_cnn.add(embedding_layer)\n",
        "    bilstm_cnn.add(Bidirectional(LSTM(LSTM_DIM, return_sequences = True)))\n",
        "    bilstm_cnn.add(Dropout(DROPOUT_RATIO))\n",
        "    bilstm_cnn.add(Conv1D(32, 3, padding = 'same', activation = 'relu'))\n",
        "    bilstm_cnn.add(MaxPool1D(2))\n",
        "    bilstm_cnn.add(Flatten())\n",
        "    bilstm_cnn.add(Dropout(DROPOUT_RATIO))\n",
        "    bilstm_cnn.add(Dense(NUM_OF_CLASSES, activation = 'softmax'))\n",
        "    adam = optimizers.adam(lr = LEARNING_RATE)\n",
        "    bilstm_cnn.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = adam,\n",
        "                  metrics = ['acc'])\n",
        "    bilstm_cnn.summary()\n",
        "    return bilstm_cnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNT8rwk_9LVM",
        "colab_type": "text"
      },
      "source": [
        "#### BiLSTM without dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMswUKAc9Nzm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bilstm_without_dropout(embedding_matrix):\n",
        "    embedding_layer = Embedding(embedding_matrix.shape[0],\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights = [embedding_matrix],\n",
        "                                input_length = MAX_SEQUENCE_LENGTH,\n",
        "                                trainable = True)\n",
        "    bilstm = Sequential()\n",
        "    bilstm.add(embedding_layer)\n",
        "    # No dropout layer\n",
        "    bilstm.add(Bidirectional(LSTM(LSTM_DIM, dropout = 0.6)))\n",
        "    bilstm.add(Dropout(0.9))\n",
        "    bilstm.add(Dense(100, activation = 'tanh'))\n",
        "    bilstm.add(Dropout(0.9))\n",
        "    bilstm.add(Dense(NUM_OF_CLASSES, activation = 'softmax'))\n",
        "    bilstm.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = 'adadelta',\n",
        "                  metrics = ['acc'])\n",
        "    bilstm.summary()\n",
        "    return bilstm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7FVonpo_N53",
        "colab_type": "text"
      },
      "source": [
        "#### CNN-GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpjqS1mp_PE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cnn_gru(embedding_matrix):\n",
        "    embedding_layer = Embedding(embedding_matrix.shape[0],\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights = [embedding_matrix],\n",
        "                                input_length = MAX_SEQUENCE_LENGTH,\n",
        "                                trainable = True)\n",
        "    cnn_gru = Sequential()\n",
        "    cnn_gru.add(embedding_layer)\n",
        "    cnn_gru.add(Conv1D(32, 3, activation = 'relu', padding = 'same'))\n",
        "    cnn_gru.add(MaxPool1D(2))\n",
        "    cnn_gru.add(GRU(LSTM_DIM))\n",
        "    cnn_gru.add(Dense(NUM_OF_CLASSES, activation = 'softmax'))\n",
        "    adam = optimizers.adam(lr = LEARNING_RATE)\n",
        "    cnn_gru.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = adam,\n",
        "                  metrics = ['acc'])\n",
        "    cnn_gru.summary()\n",
        "    return cnn_gru"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzTdS70K_uXB",
        "colab_type": "text"
      },
      "source": [
        "#### GRU_CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkfVCj-q_vvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gru_cnn(embedding_matrix):\n",
        "    embedding_layer = Embedding(embedding_matrix.shape[0],\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights = [embedding_matrix],\n",
        "                                input_length = MAX_SEQUENCE_LENGTH,\n",
        "                                trainable = True)\n",
        "    gru_cnn = Sequential()\n",
        "    gru_cnn.add(embedding_layer)\n",
        "    gru_cnn.add(GRU(LSTM_DIM, return_sequences = True))\n",
        "    gru_cnn.add(Conv1D(32, 3, activation = 'relu', padding = 'same'))\n",
        "    gru_cnn.add(MaxPool1D(2))\n",
        "    gru_cnn.add(Flatten())\n",
        "    gru_cnn.add(Dropout(0.4))\n",
        "    gru_cnn.add(Dense(NUM_OF_CLASSES, activation = 'softmax'))\n",
        "    adam = optimizers.adam(lr = LEARNING_RATE)\n",
        "    gru_cnn.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = adam,\n",
        "                  metrics = ['acc'])\n",
        "    gru_cnn.summary()\n",
        "    return gru_cnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTg2s0rHAhzA",
        "colab_type": "text"
      },
      "source": [
        "#### LSTM for first layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPkAcTkPAjgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_first_model(embedding_matrix):\n",
        "    embedding_layer = Embedding(embedding_matrix.shape[0],\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights = [embedding_matrix],\n",
        "                                input_length = MAX_SEQUENCE_LENGTH,\n",
        "                                trainable = True)\n",
        "    lstm = Sequential()\n",
        "    lstm.add(embedding_layer)\n",
        "    lstm.add(LSTM(64))\n",
        "    lstm.add(Dense(NUM_OF_CLASSES, activation = 'softmax'))\n",
        "    adam = optimizers.adam(lr = LEARNING_RATE)\n",
        "    lstm.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = adam,\n",
        "                  metrics = ['acc'])\n",
        "    lstm.summary()\n",
        "    return lstm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkGfVMq-BzWU",
        "colab_type": "text"
      },
      "source": [
        "#### Custom Model using GloVe and Sentiment-Specific word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eipnWsRB06I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_model(glove_embedding_matrix, sswe_embedding_matrix):\n",
        "    glove_embedding_layer = Embedding(glove_embedding_matrix.shape[0],\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights = [glove_embedding_matrix],\n",
        "                                input_length = MAX_SEQUENCE_LENGTH,\n",
        "                                trainable = True)\n",
        "    glove_model = Sequential()\n",
        "    glove_model.add(glove_embedding_layer)\n",
        "    glove_model.add(GRU(LSTM_DIM, return_sequences = True))\n",
        "    glove_model.add(Conv1D(32, 3, activation = 'relu', padding = 'same'))\n",
        "    glove_model.add(MaxPool1D(2))\n",
        "    glove_model.add(Flatten())\n",
        "    glove_model.add(Dropout(0.4))\n",
        "    glove_model.add(Dense(64, activation = 'softmax'))\n",
        "    adam = optimizers.adam(lr = LEARNING_RATE)\n",
        "    glove_model.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = adam,\n",
        "                  metrics = ['acc'])\n",
        "\n",
        " \n",
        "    sswe_embedding_layer = Embedding(sswe_embedding_matrix.shape[0],\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights = [sswe_embedding_matrix],\n",
        "                                input_length = MAX_SEQUENCE_LENGTH,\n",
        "                                trainable = True)    \n",
        "    sswe_model = Sequential()\n",
        "    sswe_model.add(sswe_embedding_layer)\n",
        "    sswe_model.add(GRU(LSTM_DIM, return_sequences = True))\n",
        "    sswe_model.add(Conv1D(32, 3, activation = 'relu', padding = 'same'))\n",
        "    sswe_model.add(MaxPool1D(2))\n",
        "    sswe_model.add(Flatten())\n",
        "    sswe_model.add(Dropout(0.4))\n",
        "    sswe_model.add(Dense(64, activation = 'softmax'))\n",
        "    adam = optimizers.adam(lr = LEARNING_RATE)\n",
        "    sswe_model.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = adam,\n",
        "                  metrics = ['acc'])\n",
        "    \n",
        "    \n",
        "    merged_layers = Add()([glove_model.output, sswe_model.output])\n",
        "    output_layer = Dense(4, activation = 'softmax', name = 'output_layer')(merged_layers)\n",
        "    merged_model = Model([glove_model.input, sswe_model.input], output_layer)\n",
        "    merged_model.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = adam,\n",
        "                  metrics = ['acc'])\n",
        "    merged_model.summary()\n",
        "    return merged_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2s_j_YcFA37",
        "colab_type": "text"
      },
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_rd8HgvUEF7",
        "colab_type": "code",
        "outputId": "4e954dc0-bcfb-4762-aeba-fff94613a4d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSbrQyxRFCwg",
        "colab_type": "code",
        "outputId": "12faf2ec-0040-4898-b171-7b7f0d0a1bf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Stopword removal\n",
        "# Negation should not be removed\n",
        "stopwords = set(stopwords.words('english')) - set(('not', 'no'))\n",
        "# Tag removal\n",
        "tags = ['<url>', '<email>', '<user>', '<hashtag>', '</hashtag>',\n",
        "        '<elongated>', '</elongated>', '<repeated>', '</repeated>']\n",
        "text_pre_processor = TextPreProcessor(\n",
        "    normalize  = ['url', 'email', 'user'],\n",
        "    annotate = {'hashtag', 'elongated', 'repeated'},\n",
        "    segmenter = \"twitter\",\n",
        "    corrector = \"twitter\",\n",
        "    unpack_hashtags = True,\n",
        "    unpack_contractions = True,\n",
        "    tokenizer=SocialTokenizer(lowercase = True).tokenize\n",
        ")\n",
        "def pre_process(text):\n",
        "    pre_processed_text = text_pre_processor.pre_process_doc(text)\n",
        "    return list(filter(lambda term: term not in tags and\n",
        "                                 term not in stopwords and\n",
        "                                 term not in punctuation, pre_processed_text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIubfv5pHaR5",
        "colab_type": "text"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Iw7SlC8HcOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(data_path, is_training):\n",
        "    data = pd.read_csv(data_path, encoding = 'utf-8', sep = '\\t')\n",
        "    concatenated_conversation = data[['turn1', 'turn2', 'turn3']].apply(lambda converation: ' '.join(converation), axis = 1)\n",
        "    if not is_training:\n",
        "        return data['id'], concatenated_conversation\n",
        "    else:\n",
        "        return data['id'], concatenated_conversation, data['label']\n",
        "\n",
        "def load_pre_processed_data(data_path, is_training = True):\n",
        "    if not is_training:\n",
        "        id, concatenated_conversation = load_data(data_path, is_training)\n",
        "        pre_processed_concatenated_conversation = concatenated_conversation.apply(lambda converation: pre_process(converation))\n",
        "        return id.values.tolist(), pre_processed_concatenated_conversation.values.tolist()\n",
        "    else:\n",
        "        id, concatenated_conversation, class_label = load_data(data_path, is_training)\n",
        "        pre_processed_concatenated_conversation = concatenated_conversation.apply(lambda converation: pre_process(converation))\n",
        "        emotion_label = class_label.apply(lambda emotion: emotion_to_label[emotion])\n",
        "        return id.values.tolist(), pre_processed_concatenated_conversation.values.tolist(), emotion_label.values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK92up-WNv4c",
        "colab_type": "text"
      },
      "source": [
        "### Train models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSgYGfIDNyLL",
        "colab_type": "code",
        "outputId": "f8e4f49e-90ed-4308-c593-4f054128214f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "print(\"Process training data\")\n",
        "train_indices, pre_processed_train, labels = load_pre_processed_data(train_path)\n",
        "print(\"Process test data\")\n",
        "_, pre_processed_test = load_pre_processed_data(test_path, is_training = False)\n",
        "print(\"Process validation data\")\n",
        "_, pre_processed_validation, validation_classes = load_pre_processed_data(validation_path)\n",
        "\n",
        "print(\"Tokenize\")\n",
        "tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(pre_processed_train)\n",
        "train_sequences = tokenizer.texts_to_sequences(pre_processed_train)\n",
        "test_sequences = tokenizer.texts_to_sequences(pre_processed_test)\n",
        "validation_sequences = tokenizer.texts_to_sequences(pre_processed_validation)\n",
        "word_index = tokenizer.word_index\n",
        "print(\"%s unique tokens found.\" % len(word_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Process training data\n",
            "Process test data\n",
            "Process validation data\n",
            "Tokenize\n",
            "14162 unique tokens found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOr-RDGt6K_h",
        "colab_type": "text"
      },
      "source": [
        "#### GloVe Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCkawyOA5jws",
        "colab_type": "code",
        "outputId": "959f89d1-eb07-4384-d233-7dd7b84f220f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"Generate GloVe embedding matrix\")\n",
        "glove_embedding_matrix = create_embedding_matrix(word_index, os.path.join(glove_path, 'glove.840B.300d.txt'))\n",
        "padded_training_sequences = pad_sequences(train_sequences, maxlen = MAX_SEQUENCE_LENGTH)\n",
        "labels = to_categorical(np.asarray(labels))\n",
        "padded_validation_sequences = pad_sequences(validation_sequences, maxlen = MAX_SEQUENCE_LENGTH)\n",
        "validation_labels = to_categorical(np.asarray(validation_classes))\n",
        "np.random.shuffle(train_indices)\n",
        "padded_training_sequences = padded_training_sequences[train_indices]\n",
        "labels = labels[train_indices]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generate GloVe embedding matrix\n",
            "2196016 embedding vectors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rByKgHl2SIa",
        "colab_type": "text"
      },
      "source": [
        "##### GRU-CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfpYk_WlQcn0",
        "colab_type": "code",
        "outputId": "a4756e2c-caa9-4e39-cd38-45e29e4bf21d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"Building GRU-CNN model\")\n",
        "cbks = [ModelCheckpoint('./gru_cnn_glove_model.h5', verbose = 1, monitor = 'val_loss', save_best_only = True, mode = 'auto'),\n",
        "        EarlyStopping(monitor = 'val_loss', patience = 2),\n",
        "        ComputeMetricsCallback((padded_validation_sequences, validation_labels))]\n",
        "gru_cnn_model = gru_cnn(glove_embedding_matrix)\n",
        "gru_cnn_model.fit(padded_training_sequences, labels, validation_data=(padded_validation_sequences, validation_labels), epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, shuffle=True, callbacks=cbks)\n",
        "gru_cnn_model = load_model('./gru_cnn_glove_model.h5')\n",
        "print(\"Generating prediction file\")\n",
        "generate_result_file(gru_cnn_model, test_sequences, result_file_name = \"gru_cnn_glove_predictions.txt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generate embedding matrix\n",
            "2196016 embedding vectors\n",
            "Building model\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 64, 300)           4248900   \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 64, 300)           540900    \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 64, 32)            28832     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 32, 32)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 4100      \n",
            "=================================================================\n",
            "Total params: 4,822,732\n",
            "Trainable params: 4,822,732\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 30160 samples, validate on 2755 samples\n",
            "Epoch 1/10\n",
            "30160/30160 [==============================] - 22s 746us/step - loss: 0.7309 - acc: 0.7138 - val_loss: 0.3449 - val_acc: 0.8838\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.34491, saving model to ./gru_cnn.h5\n",
            "True Positives per class :  [2116.   97.   99.  123.]\n",
            "False Positives per class :  [86. 61. 78. 95.]\n",
            "False Negatives per class :  [222.  45.  26.  27.]\n",
            "Class happy : Precision : 0.614, Recall : 0.683, F1-Score : 0.647\n",
            "Class sad : Precision : 0.559, Recall : 0.792, F1-Score : 0.656\n",
            "Class angry : Precision : 0.564, Recall : 0.820, F1-Score : 0.668\n",
            "Macro Precision : 0.579, Macro Recall : 0.765, Macro F1-Score : 0.659\n",
            "Accuracy : 0.884, Micro Precision : 0.577, Micro Recall : 0.765, Micro F1-Score : 0.658\n",
            "Epoch 2/10\n",
            "30160/30160 [==============================] - 21s 682us/step - loss: 0.3946 - acc: 0.8654 - val_loss: 0.3562 - val_acc: 0.8773\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.34491\n",
            "True Positives per class :  [2087.  101.  104.  125.]\n",
            "False Positives per class :  [ 76.  63. 113.  86.]\n",
            "False Negatives per class :  [251.  41.  21.  25.]\n",
            "Class happy : Precision : 0.616, Recall : 0.711, F1-Score : 0.660\n",
            "Class sad : Precision : 0.479, Recall : 0.832, F1-Score : 0.608\n",
            "Class angry : Precision : 0.592, Recall : 0.833, F1-Score : 0.693\n",
            "Macro Precision : 0.563, Macro Recall : 0.792, Macro F1-Score : 0.658\n",
            "Accuracy : 0.877, Micro Precision : 0.557, Micro Recall : 0.791, Micro F1-Score : 0.654\n",
            "Epoch 3/10\n",
            "30160/30160 [==============================] - 20s 675us/step - loss: 0.3164 - acc: 0.8929 - val_loss: 0.3538 - val_acc: 0.8722\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.34491\n",
            "True Positives per class :  [2070.  111.   98.  124.]\n",
            "False Positives per class :  [ 73. 114.  80.  85.]\n",
            "False Negatives per class :  [268.  31.  27.  26.]\n",
            "Class happy : Precision : 0.493, Recall : 0.782, F1-Score : 0.605\n",
            "Class sad : Precision : 0.551, Recall : 0.784, F1-Score : 0.647\n",
            "Class angry : Precision : 0.593, Recall : 0.827, F1-Score : 0.691\n",
            "Macro Precision : 0.546, Macro Recall : 0.797, Macro F1-Score : 0.648\n",
            "Accuracy : 0.872, Micro Precision : 0.544, Micro Recall : 0.799, Micro F1-Score : 0.647\n",
            "Epoch 4/10\n",
            "30160/30160 [==============================] - 20s 674us/step - loss: 0.2576 - acc: 0.9117 - val_loss: 0.3276 - val_acc: 0.8933\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.34491 to 0.32763, saving model to ./gru_cnn.h5\n",
            "True Positives per class :  [2139.   98.   98.  126.]\n",
            "False Positives per class :  [87. 65. 61. 81.]\n",
            "False Negatives per class :  [199.  44.  27.  24.]\n",
            "Class happy : Precision : 0.601, Recall : 0.690, F1-Score : 0.643\n",
            "Class sad : Precision : 0.616, Recall : 0.784, F1-Score : 0.690\n",
            "Class angry : Precision : 0.609, Recall : 0.840, F1-Score : 0.706\n",
            "Macro Precision : 0.609, Macro Recall : 0.771, Macro F1-Score : 0.680\n",
            "Accuracy : 0.893, Micro Precision : 0.609, Micro Recall : 0.772, Micro F1-Score : 0.681\n",
            "Epoch 5/10\n",
            "30160/30160 [==============================] - 20s 675us/step - loss: 0.2067 - acc: 0.9325 - val_loss: 0.3649 - val_acc: 0.8788\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.32763\n",
            "True Positives per class :  [2101.  103.   96.  121.]\n",
            "False Positives per class :  [86. 84. 72. 92.]\n",
            "False Negatives per class :  [237.  39.  29.  29.]\n",
            "Class happy : Precision : 0.551, Recall : 0.725, F1-Score : 0.626\n",
            "Class sad : Precision : 0.571, Recall : 0.768, F1-Score : 0.655\n",
            "Class angry : Precision : 0.568, Recall : 0.807, F1-Score : 0.667\n",
            "Macro Precision : 0.563, Macro Recall : 0.767, Macro F1-Score : 0.650\n",
            "Accuracy : 0.879, Micro Precision : 0.563, Micro Recall : 0.767, Micro F1-Score : 0.650\n",
            "Epoch 6/10\n",
            "30160/30160 [==============================] - 21s 686us/step - loss: 0.1624 - acc: 0.9466 - val_loss: 0.4502 - val_acc: 0.8617\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.32763\n",
            "True Positives per class :  [2047.  108.   96.  123.]\n",
            "False Positives per class :  [ 76. 108.  91. 106.]\n",
            "False Negatives per class :  [291.  34.  29.  27.]\n",
            "Class happy : Precision : 0.500, Recall : 0.761, F1-Score : 0.603\n",
            "Class sad : Precision : 0.513, Recall : 0.768, F1-Score : 0.615\n",
            "Class angry : Precision : 0.537, Recall : 0.820, F1-Score : 0.649\n",
            "Macro Precision : 0.517, Macro Recall : 0.783, Macro F1-Score : 0.623\n",
            "Accuracy : 0.862, Micro Precision : 0.517, Micro Recall : 0.784, Micro F1-Score : 0.623\n",
            "Epoch 7/10\n",
            "30160/30160 [==============================] - 20s 675us/step - loss: 0.1283 - acc: 0.9584 - val_loss: 0.5320 - val_acc: 0.8490\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.32763\n",
            "True Positives per class :  [2013.  104.   94.  128.]\n",
            "False Positives per class :  [ 66. 109.  82. 159.]\n",
            "False Negatives per class :  [325.  38.  31.  22.]\n",
            "Class happy : Precision : 0.488, Recall : 0.732, F1-Score : 0.586\n",
            "Class sad : Precision : 0.534, Recall : 0.752, F1-Score : 0.625\n",
            "Class angry : Precision : 0.446, Recall : 0.853, F1-Score : 0.586\n",
            "Macro Precision : 0.489, Macro Recall : 0.779, Macro F1-Score : 0.601\n",
            "Accuracy : 0.849, Micro Precision : 0.482, Micro Recall : 0.782, Micro F1-Score : 0.597\n",
            "Epoch 8/10\n",
            "30160/30160 [==============================] - 20s 673us/step - loss: 0.1060 - acc: 0.9660 - val_loss: 0.5782 - val_acc: 0.8566\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.32763\n",
            "True Positives per class :  [2032.  110.   95.  123.]\n",
            "False Positives per class :  [ 72. 130.  81. 112.]\n",
            "False Negatives per class :  [306.  32.  30.  27.]\n",
            "Class happy : Precision : 0.458, Recall : 0.775, F1-Score : 0.576\n",
            "Class sad : Precision : 0.540, Recall : 0.760, F1-Score : 0.631\n",
            "Class angry : Precision : 0.523, Recall : 0.820, F1-Score : 0.639\n",
            "Macro Precision : 0.507, Macro Recall : 0.785, Macro F1-Score : 0.616\n",
            "Accuracy : 0.857, Micro Precision : 0.504, Micro Recall : 0.787, Micro F1-Score : 0.614\n",
            "Epoch 9/10\n",
            "30160/30160 [==============================] - 20s 674us/step - loss: 0.0850 - acc: 0.9728 - val_loss: 0.5774 - val_acc: 0.8675\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.32763\n",
            "True Positives per class :  [2079.   94.   97.  120.]\n",
            "False Positives per class :  [ 89.  54.  91. 131.]\n",
            "False Negatives per class :  [259.  48.  28.  30.]\n",
            "Class happy : Precision : 0.635, Recall : 0.662, F1-Score : 0.648\n",
            "Class sad : Precision : 0.516, Recall : 0.776, F1-Score : 0.620\n",
            "Class angry : Precision : 0.478, Recall : 0.800, F1-Score : 0.599\n",
            "Macro Precision : 0.543, Macro Recall : 0.746, Macro F1-Score : 0.629\n",
            "Accuracy : 0.868, Micro Precision : 0.530, Micro Recall : 0.746, Micro F1-Score : 0.620\n",
            "Epoch 10/10\n",
            "30160/30160 [==============================] - 20s 674us/step - loss: 0.0683 - acc: 0.9781 - val_loss: 0.7558 - val_acc: 0.8505\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.32763\n",
            "True Positives per class :  [2023.  109.   92.  119.]\n",
            "False Positives per class :  [ 78. 124.  73. 137.]\n",
            "False Negatives per class :  [315.  33.  33.  31.]\n",
            "Class happy : Precision : 0.468, Recall : 0.768, F1-Score : 0.581\n",
            "Class sad : Precision : 0.558, Recall : 0.736, F1-Score : 0.634\n",
            "Class angry : Precision : 0.465, Recall : 0.793, F1-Score : 0.586\n",
            "Macro Precision : 0.497, Macro Recall : 0.766, Macro F1-Score : 0.603\n",
            "Accuracy : 0.850, Micro Precision : 0.489, Micro Recall : 0.767, Micro F1-Score : 0.598\n",
            "Generating prediction file\n",
            "Model parameters: LSTM Dim : 300, Dropout : 0.2, Batch_size : 200, Learning rate : 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r_QfCla2cBB",
        "colab_type": "text"
      },
      "source": [
        "##### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmqRoTp1vh8C",
        "colab_type": "code",
        "outputId": "5925368b-d858-46bd-db44-7e8b7628d91c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"Building LSTM model\")\n",
        "callback = [ModelCheckpoint('./lstm_glove_model.h5', verbose = 1, monitor = 'val_loss', save_best_only = True, mode = 'auto'),\n",
        "        EarlyStopping(monitor = 'val_loss', patience = 2),\n",
        "        ComputeMetricsCallback((padded_validation_sequences, validation_labels))]\n",
        "model = lstm_first_model(glove_embedding_matrix)\n",
        "model.fit(padded_training_sequences, labels, validation_data = (padded_validation_sequences, validation_labels), epochs = NUM_EPOCHS, batch_size = BATCH_SIZE, shuffle = True, callbacks = callback)\n",
        "model = load_model('./lstm_glove_model.h5')\n",
        "print(\"Generating prediction file\")\n",
        "generate_result_file(model, test_sequences, result_file_name = \"lstm_glove_predictions.txt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building LSTM model\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 64, 300)           4248900   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 4,342,600\n",
            "Trainable params: 4,342,600\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 30160 samples, validate on 2755 samples\n",
            "Epoch 1/10\n",
            "30160/30160 [==============================] - 22s 722us/step - loss: 0.6373 - acc: 0.7549 - val_loss: 0.3624 - val_acc: 0.8711\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.36243, saving model to ./lstm_glove_model.h5\n",
            "True Positives per class :  [2078.   98.   94.  130.]\n",
            "False Positives per class :  [ 81.  73.  86. 115.]\n",
            "False Negatives per class :  [260.  44.  31.  20.]\n",
            "Class happy : Precision : 0.573, Recall : 0.690, F1-Score : 0.626\n",
            "Class sad : Precision : 0.522, Recall : 0.752, F1-Score : 0.616\n",
            "Class angry : Precision : 0.531, Recall : 0.867, F1-Score : 0.658\n",
            "Macro Precision : 0.542, Macro Recall : 0.770, Macro F1-Score : 0.636\n",
            "Accuracy : 0.871, Micro Precision : 0.540, Micro Recall : 0.772, Micro F1-Score : 0.636\n",
            "Epoch 2/10\n",
            "30160/30160 [==============================] - 21s 684us/step - loss: 0.3443 - acc: 0.8779 - val_loss: 0.4005 - val_acc: 0.8461\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.36243\n",
            "True Positives per class :  [1990.  110.   97.  134.]\n",
            "False Positives per class :  [ 64. 107. 101. 152.]\n",
            "False Negatives per class :  [348.  32.  28.  16.]\n",
            "Class happy : Precision : 0.507, Recall : 0.775, F1-Score : 0.613\n",
            "Class sad : Precision : 0.490, Recall : 0.776, F1-Score : 0.601\n",
            "Class angry : Precision : 0.469, Recall : 0.893, F1-Score : 0.615\n",
            "Macro Precision : 0.488, Macro Recall : 0.815, Macro F1-Score : 0.611\n",
            "Accuracy : 0.846, Micro Precision : 0.486, Micro Recall : 0.818, Micro F1-Score : 0.610\n",
            "Epoch 3/10\n",
            "30160/30160 [==============================] - 21s 692us/step - loss: 0.2756 - acc: 0.9031 - val_loss: 0.4364 - val_acc: 0.8377\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.36243\n",
            "True Positives per class :  [1967.  112.  103.  126.]\n",
            "False Positives per class :  [ 60. 127. 141. 119.]\n",
            "False Negatives per class :  [371.  30.  22.  24.]\n",
            "Class happy : Precision : 0.469, Recall : 0.789, F1-Score : 0.588\n",
            "Class sad : Precision : 0.422, Recall : 0.824, F1-Score : 0.558\n",
            "Class angry : Precision : 0.514, Recall : 0.840, F1-Score : 0.638\n",
            "Macro Precision : 0.468, Macro Recall : 0.818, Macro F1-Score : 0.596\n",
            "Accuracy : 0.838, Micro Precision : 0.468, Micro Recall : 0.818, Micro F1-Score : 0.596\n",
            "Epoch 4/10\n",
            "30160/30160 [==============================] - 21s 689us/step - loss: 0.2294 - acc: 0.9227 - val_loss: 0.4210 - val_acc: 0.8490\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.36243\n",
            "True Positives per class :  [2001.  110.  101.  127.]\n",
            "False Positives per class :  [ 66. 108. 112. 130.]\n",
            "False Negatives per class :  [337.  32.  24.  23.]\n",
            "Class happy : Precision : 0.505, Recall : 0.775, F1-Score : 0.611\n",
            "Class sad : Precision : 0.474, Recall : 0.808, F1-Score : 0.598\n",
            "Class angry : Precision : 0.494, Recall : 0.847, F1-Score : 0.624\n",
            "Macro Precision : 0.491, Macro Recall : 0.810, Macro F1-Score : 0.611\n",
            "Accuracy : 0.849, Micro Precision : 0.491, Micro Recall : 0.811, Micro F1-Score : 0.612\n",
            "Epoch 5/10\n",
            "30160/30160 [==============================] - 21s 681us/step - loss: 0.1866 - acc: 0.9373 - val_loss: 0.3893 - val_acc: 0.8672\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.36243\n",
            "True Positives per class :  [2067.  107.   97.  118.]\n",
            "False Positives per class :  [ 81. 108.  88.  89.]\n",
            "False Negatives per class :  [271.  35.  28.  32.]\n",
            "Class happy : Precision : 0.498, Recall : 0.754, F1-Score : 0.599\n",
            "Class sad : Precision : 0.524, Recall : 0.776, F1-Score : 0.626\n",
            "Class angry : Precision : 0.570, Recall : 0.787, F1-Score : 0.661\n",
            "Macro Precision : 0.531, Macro Recall : 0.772, Macro F1-Score : 0.629\n",
            "Accuracy : 0.867, Micro Precision : 0.530, Micro Recall : 0.772, Micro F1-Score : 0.629\n",
            "Epoch 6/10\n",
            "30160/30160 [==============================] - 21s 682us/step - loss: 0.1525 - acc: 0.9507 - val_loss: 0.4480 - val_acc: 0.8563\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.36243\n",
            "True Positives per class :  [2038.  107.   93.  121.]\n",
            "False Positives per class :  [ 78. 116.  76. 126.]\n",
            "False Negatives per class :  [300.  35.  32.  29.]\n",
            "Class happy : Precision : 0.480, Recall : 0.754, F1-Score : 0.586\n",
            "Class sad : Precision : 0.550, Recall : 0.744, F1-Score : 0.633\n",
            "Class angry : Precision : 0.490, Recall : 0.807, F1-Score : 0.610\n",
            "Macro Precision : 0.507, Macro Recall : 0.768, Macro F1-Score : 0.611\n",
            "Accuracy : 0.856, Micro Precision : 0.502, Micro Recall : 0.770, Micro F1-Score : 0.608\n",
            "Epoch 7/10\n",
            "30160/30160 [==============================] - 21s 690us/step - loss: 0.1304 - acc: 0.9578 - val_loss: 0.5024 - val_acc: 0.8425\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.36243\n",
            "True Positives per class :  [1989.  108.   98.  126.]\n",
            "False Positives per class :  [ 66. 125. 109. 134.]\n",
            "False Negatives per class :  [349.  34.  27.  24.]\n",
            "Class happy : Precision : 0.464, Recall : 0.761, F1-Score : 0.576\n",
            "Class sad : Precision : 0.473, Recall : 0.784, F1-Score : 0.590\n",
            "Class angry : Precision : 0.485, Recall : 0.840, F1-Score : 0.615\n",
            "Macro Precision : 0.474, Macro Recall : 0.795, Macro F1-Score : 0.594\n",
            "Accuracy : 0.842, Micro Precision : 0.474, Micro Recall : 0.796, Micro F1-Score : 0.594\n",
            "Epoch 8/10\n",
            "30160/30160 [==============================] - 21s 682us/step - loss: 0.1112 - acc: 0.9645 - val_loss: 0.4595 - val_acc: 0.8610\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.36243\n",
            "True Positives per class :  [2061.  104.   93.  114.]\n",
            "False Positives per class :  [ 88. 124.  82.  89.]\n",
            "False Negatives per class :  [277.  38.  32.  36.]\n",
            "Class happy : Precision : 0.456, Recall : 0.732, F1-Score : 0.562\n",
            "Class sad : Precision : 0.531, Recall : 0.744, F1-Score : 0.620\n",
            "Class angry : Precision : 0.562, Recall : 0.760, F1-Score : 0.646\n",
            "Macro Precision : 0.516, Macro Recall : 0.745, Macro F1-Score : 0.610\n",
            "Accuracy : 0.861, Micro Precision : 0.513, Micro Recall : 0.746, Micro F1-Score : 0.608\n",
            "Epoch 9/10\n",
            "30160/30160 [==============================] - 20s 670us/step - loss: 0.0919 - acc: 0.9718 - val_loss: 0.5506 - val_acc: 0.8396\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.36243\n",
            "True Positives per class :  [2002.  100.   92.  119.]\n",
            "False Positives per class :  [ 87. 113.  90. 152.]\n",
            "False Negatives per class :  [336.  42.  33.  31.]\n",
            "Class happy : Precision : 0.469, Recall : 0.704, F1-Score : 0.563\n",
            "Class sad : Precision : 0.505, Recall : 0.736, F1-Score : 0.599\n",
            "Class angry : Precision : 0.439, Recall : 0.793, F1-Score : 0.565\n",
            "Macro Precision : 0.471, Macro Recall : 0.745, Macro F1-Score : 0.577\n",
            "Accuracy : 0.840, Micro Precision : 0.467, Micro Recall : 0.746, Micro F1-Score : 0.574\n",
            "Epoch 10/10\n",
            "30160/30160 [==============================] - 20s 676us/step - loss: 0.0793 - acc: 0.9749 - val_loss: 0.5756 - val_acc: 0.8421\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.36243\n",
            "True Positives per class :  [2005.  101.   93.  121.]\n",
            "False Positives per class :  [ 84. 125.  82. 144.]\n",
            "False Negatives per class :  [333.  41.  32.  29.]\n",
            "Class happy : Precision : 0.447, Recall : 0.711, F1-Score : 0.549\n",
            "Class sad : Precision : 0.531, Recall : 0.744, F1-Score : 0.620\n",
            "Class angry : Precision : 0.457, Recall : 0.807, F1-Score : 0.583\n",
            "Macro Precision : 0.478, Macro Recall : 0.754, Macro F1-Score : 0.585\n",
            "Accuracy : 0.842, Micro Precision : 0.473, Micro Recall : 0.755, Micro F1-Score : 0.582\n",
            "Generating prediction file\n",
            "Model parameters: LSTM Dim : 300, Dropout : 0.2, Batch_size : 200, Learning rate : 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3aZWmdE6UiJ",
        "colab_type": "text"
      },
      "source": [
        "#### Sentiment-Specific embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNmE7CvK6r8j",
        "colab_type": "code",
        "outputId": "99fe103d-d36b-401b-9fec-fd1144415b60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"Generating Sentiment-Specific word embedding matrix\")\n",
        "sswe_embedding_matrix = create_embedding_matrix(word_index, os.path.join(sswe_path, 'sswe-r.txt'))\n",
        "padded_training_sequences = pad_sequences(train_sequences, maxlen = MAX_SEQUENCE_LENGTH)\n",
        "labels = to_categorical(np.asarray(labels))\n",
        "padded_validation_sequences = pad_sequences(validation_sequences, maxlen = MAX_SEQUENCE_LENGTH)\n",
        "validation_labels = to_categorical(np.asarray(validation_classes))\n",
        "np.random.shuffle(train_indices)\n",
        "padded_training_sequences = padded_training_sequences[train_indices]\n",
        "labels = labels[train_indices]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating Sentiment-Specific word embedding matrix\n",
            "137052 embedding vectors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apds_wOI71Yq",
        "colab_type": "text"
      },
      "source": [
        "##### GRU-CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLe-Q2Q66bzw",
        "colab_type": "code",
        "outputId": "be379a23-8c49-4521-8ecc-dcc3b732bfa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"Building GRU-CNN model\")\n",
        "callback = [ModelCheckpoint('./gru_cnn_sswe_model.h5', verbose = 1, monitor = 'val_loss', save_best_only = True, mode = 'auto'),\n",
        "        EarlyStopping(monitor = 'val_loss', patience = 2),\n",
        "        ComputeMetricsCallback((padded_validation_sequences, validation_labels))]\n",
        "gru_cnn_sswe_model = gru_cnn(sswe_embedding_matrix)\n",
        "gru_cnn_sswe_model.fit(padded_training_sequences, labels, validation_data = (padded_validation_sequences, validation_labels), epochs = NUM_EPOCHS, batch_size = BATCH_SIZE, shuffle = True, callbacks = callback)\n",
        "gru_cnn_sswe_model = load_model('./gru_cnn_sswe_model.h5')\n",
        "print(\"Generating prediction file\")\n",
        "generate_result_file(model, test_sequences, result_file_name = \"gru_cnn_sswe_predictions.txt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building GRU-CNN model\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 64, 300)           4248900   \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 64, 300)           540900    \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 64, 32)            28832     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 32, 32)            0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 4)                 4100      \n",
            "=================================================================\n",
            "Total params: 4,822,732\n",
            "Trainable params: 4,822,732\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 30160 samples, validate on 2755 samples\n",
            "Epoch 1/10\n",
            "30160/30160 [==============================] - 23s 779us/step - loss: 0.8065 - acc: 0.6835 - val_loss: 0.3674 - val_acc: 0.8799\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.36742, saving model to ./gru_cnn_sswe_model.h5\n",
            "True Positives per class :  [2098.  109.   98.  119.]\n",
            "False Positives per class :  [ 80. 103.  74.  74.]\n",
            "False Negatives per class :  [240.  33.  27.  31.]\n",
            "Class happy : Precision : 0.514, Recall : 0.768, F1-Score : 0.616\n",
            "Class sad : Precision : 0.570, Recall : 0.784, F1-Score : 0.660\n",
            "Class angry : Precision : 0.617, Recall : 0.793, F1-Score : 0.694\n",
            "Macro Precision : 0.567, Macro Recall : 0.782, Macro F1-Score : 0.657\n",
            "Accuracy : 0.880, Micro Precision : 0.565, Micro Recall : 0.782, Micro F1-Score : 0.656\n",
            "Epoch 2/10\n",
            "30160/30160 [==============================] - 21s 698us/step - loss: 0.3975 - acc: 0.8639 - val_loss: 0.4325 - val_acc: 0.8548\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.36742\n",
            "True Positives per class :  [2016.  109.  100.  130.]\n",
            "False Positives per class :  [ 65. 119. 105. 111.]\n",
            "False Negatives per class :  [322.  33.  25.  20.]\n",
            "Class happy : Precision : 0.478, Recall : 0.768, F1-Score : 0.589\n",
            "Class sad : Precision : 0.488, Recall : 0.800, F1-Score : 0.606\n",
            "Class angry : Precision : 0.539, Recall : 0.867, F1-Score : 0.665\n",
            "Macro Precision : 0.502, Macro Recall : 0.811, Macro F1-Score : 0.620\n",
            "Accuracy : 0.855, Micro Precision : 0.503, Micro Recall : 0.813, Micro F1-Score : 0.621\n",
            "Epoch 3/10\n",
            "30160/30160 [==============================] - 21s 699us/step - loss: 0.3052 - acc: 0.8972 - val_loss: 0.4502 - val_acc: 0.8432\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.36742\n",
            "True Positives per class :  [1998.  109.   98.  118.]\n",
            "False Positives per class :  [ 77. 139. 115. 101.]\n",
            "False Negatives per class :  [340.  33.  27.  32.]\n",
            "Class happy : Precision : 0.440, Recall : 0.768, F1-Score : 0.559\n",
            "Class sad : Precision : 0.460, Recall : 0.784, F1-Score : 0.580\n",
            "Class angry : Precision : 0.539, Recall : 0.787, F1-Score : 0.640\n",
            "Macro Precision : 0.479, Macro Recall : 0.779, Macro F1-Score : 0.594\n",
            "Accuracy : 0.843, Micro Precision : 0.478, Micro Recall : 0.779, Micro F1-Score : 0.593\n",
            "Epoch 4/10\n",
            "30160/30160 [==============================] - 21s 699us/step - loss: 0.2439 - acc: 0.9188 - val_loss: 0.4320 - val_acc: 0.8657\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.36742\n",
            "True Positives per class :  [2057.  107.   95.  126.]\n",
            "False Positives per class :  [ 76.  94.  88. 112.]\n",
            "False Negatives per class :  [281.  35.  30.  24.]\n",
            "Class happy : Precision : 0.532, Recall : 0.754, F1-Score : 0.624\n",
            "Class sad : Precision : 0.519, Recall : 0.760, F1-Score : 0.617\n",
            "Class angry : Precision : 0.529, Recall : 0.840, F1-Score : 0.649\n",
            "Macro Precision : 0.527, Macro Recall : 0.785, Macro F1-Score : 0.630\n",
            "Accuracy : 0.866, Micro Precision : 0.527, Micro Recall : 0.787, Micro F1-Score : 0.631\n",
            "Epoch 5/10\n",
            "30160/30160 [==============================] - 21s 696us/step - loss: 0.2003 - acc: 0.9347 - val_loss: 0.4809 - val_acc: 0.8490\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.36742\n",
            "True Positives per class :  [2016.  107.   92.  124.]\n",
            "False Positives per class :  [ 78. 106.  99. 133.]\n",
            "False Negatives per class :  [322.  35.  33.  26.]\n",
            "Class happy : Precision : 0.502, Recall : 0.754, F1-Score : 0.603\n",
            "Class sad : Precision : 0.482, Recall : 0.736, F1-Score : 0.582\n",
            "Class angry : Precision : 0.482, Recall : 0.827, F1-Score : 0.609\n",
            "Macro Precision : 0.489, Macro Recall : 0.772, Macro F1-Score : 0.599\n",
            "Accuracy : 0.849, Micro Precision : 0.489, Micro Recall : 0.775, Micro F1-Score : 0.599\n",
            "Epoch 6/10\n",
            "30160/30160 [==============================] - 21s 699us/step - loss: 0.1659 - acc: 0.9456 - val_loss: 0.5348 - val_acc: 0.8396\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.36742\n",
            "True Positives per class :  [1989.  107.   95.  122.]\n",
            "False Positives per class :  [ 76. 105. 109. 152.]\n",
            "False Negatives per class :  [349.  35.  30.  28.]\n",
            "Class happy : Precision : 0.505, Recall : 0.754, F1-Score : 0.605\n",
            "Class sad : Precision : 0.466, Recall : 0.760, F1-Score : 0.578\n",
            "Class angry : Precision : 0.445, Recall : 0.813, F1-Score : 0.575\n",
            "Macro Precision : 0.472, Macro Recall : 0.776, Macro F1-Score : 0.587\n",
            "Accuracy : 0.840, Micro Precision : 0.470, Micro Recall : 0.777, Micro F1-Score : 0.585\n",
            "Epoch 7/10\n",
            "30160/30160 [==============================] - 21s 690us/step - loss: 0.1403 - acc: 0.9536 - val_loss: 0.5518 - val_acc: 0.8417\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.36742\n",
            "True Positives per class :  [1998.  106.   98.  117.]\n",
            "False Positives per class :  [ 78. 110. 115. 133.]\n",
            "False Negatives per class :  [340.  36.  27.  33.]\n",
            "Class happy : Precision : 0.491, Recall : 0.746, F1-Score : 0.592\n",
            "Class sad : Precision : 0.460, Recall : 0.784, F1-Score : 0.580\n",
            "Class angry : Precision : 0.468, Recall : 0.780, F1-Score : 0.585\n",
            "Macro Precision : 0.473, Macro Recall : 0.770, Macro F1-Score : 0.586\n",
            "Accuracy : 0.842, Micro Precision : 0.473, Micro Recall : 0.770, Micro F1-Score : 0.586\n",
            "Epoch 8/10\n",
            "30160/30160 [==============================] - 21s 697us/step - loss: 0.1217 - acc: 0.9597 - val_loss: 0.6349 - val_acc: 0.8370\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.36742\n",
            "True Positives per class :  [1987.  108.   91.  120.]\n",
            "False Positives per class :  [ 78. 121.  82. 168.]\n",
            "False Negatives per class :  [351.  34.  34.  30.]\n",
            "Class happy : Precision : 0.472, Recall : 0.761, F1-Score : 0.582\n",
            "Class sad : Precision : 0.526, Recall : 0.728, F1-Score : 0.611\n",
            "Class angry : Precision : 0.417, Recall : 0.800, F1-Score : 0.548\n",
            "Macro Precision : 0.471, Macro Recall : 0.763, Macro F1-Score : 0.583\n",
            "Accuracy : 0.837, Micro Precision : 0.462, Micro Recall : 0.765, Micro F1-Score : 0.576\n",
            "Epoch 9/10\n",
            "30160/30160 [==============================] - 21s 702us/step - loss: 0.1054 - acc: 0.9648 - val_loss: 0.6577 - val_acc: 0.8417\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.36742\n",
            "True Positives per class :  [2007.   98.   94.  120.]\n",
            "False Positives per class :  [ 83.  93.  92. 168.]\n",
            "False Negatives per class :  [331.  44.  31.  30.]\n",
            "Class happy : Precision : 0.513, Recall : 0.690, F1-Score : 0.589\n",
            "Class sad : Precision : 0.505, Recall : 0.752, F1-Score : 0.605\n",
            "Class angry : Precision : 0.417, Recall : 0.800, F1-Score : 0.548\n",
            "Macro Precision : 0.478, Macro Recall : 0.747, Macro F1-Score : 0.583\n",
            "Accuracy : 0.842, Micro Precision : 0.469, Micro Recall : 0.748, Micro F1-Score : 0.577\n",
            "Epoch 10/10\n",
            "30160/30160 [==============================] - 21s 703us/step - loss: 0.0918 - acc: 0.9695 - val_loss: 0.7618 - val_acc: 0.8247\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.36742\n",
            "True Positives per class :  [1956.  105.   92.  119.]\n",
            "False Positives per class :  [ 81. 128.  88. 186.]\n",
            "False Negatives per class :  [382.  37.  33.  31.]\n",
            "Class happy : Precision : 0.451, Recall : 0.739, F1-Score : 0.560\n",
            "Class sad : Precision : 0.511, Recall : 0.736, F1-Score : 0.603\n",
            "Class angry : Precision : 0.390, Recall : 0.793, F1-Score : 0.523\n",
            "Macro Precision : 0.451, Macro Recall : 0.756, Macro F1-Score : 0.565\n",
            "Accuracy : 0.825, Micro Precision : 0.440, Micro Recall : 0.758, Micro F1-Score : 0.557\n",
            "Generating prediction file\n",
            "Model parameters: LSTM Dim : 300, Dropout : 0.2, Batch_size : 200, Learning rate : 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gsPhzlH966n",
        "colab_type": "text"
      },
      "source": [
        "##### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-mBKAT598YF",
        "colab_type": "code",
        "outputId": "0acd6c81-a5c7-42cd-a0e4-a03e829ec4e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        }
      },
      "source": [
        "print(\"Building LSTM model\")\n",
        "callback = [ModelCheckpoint('./lstm_sswe_model.h5', verbose = 1, monitor = 'val_loss', save_best_only = True, mode='auto'),\n",
        "        EarlyStopping(monitor = 'val_loss', patience = 2),\n",
        "        ComputeMetricsCallback((padded_validation_sequences, validation_labels))]\n",
        "lstm_sswe_model = lstm_first_model(sswe_embedding_matrix)\n",
        "lstm_sswe_model.fit(padded_training_sequences, labels, validation_data = (padded_validation_sequences, validation_labels), epochs = NUM_EPOCHS, batch_size = BATCH_SIZE, shuffle = True, callbacks = callback)\n",
        "lstm_sswe_model = load_model('./lstm_sswe_model.h5')\n",
        "print(\"Generating prediction file\")\n",
        "generate_result_file(model, test_sequences, result_file_name = \"lstm_sswe_predictions.txt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building LSTM model\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 64, 300)           4248900   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 4,342,600\n",
            "Trainable params: 4,342,600\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-a42ce669504a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         ComputeMetricsCallback((padded_validation_sequences, validation_labels))]\n\u001b[1;32m      5\u001b[0m \u001b[0mlstm_sswe_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_first_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msswe_embedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlstm_sswe_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_training_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpadded_validation_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mlstm_sswe_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./lstm_sswe_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating prediction file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have 2 dimensions, but got array with shape (30160, 4, 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN-9jfM7_kyi",
        "colab_type": "text"
      },
      "source": [
        "#### Combined GloVe and SSWE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN2_uWrz_pLK",
        "colab_type": "code",
        "outputId": "6a166cb3-cf90-486b-cb9b-d46086a8f418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "source": [
        "model = load_model('./lstm_glove_model.h5')\n",
        "print(model.layers[2].get_weights()[0].shape)\n",
        "lstm_glove_weights = np.zeros(shape = (64,1))\n",
        "x = model.layers[2].get_weights()[0]\n",
        "lstm_glove_weights = np.sum(x, axis = 1)\n",
        "print(lstm_glove_weights.shape)\n",
        "lstm_glove_weights = lstm_glove_weights.reshape(lstm_glove_weights.shape[0], 1)\n",
        "print(lstm_glove_weights.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "(64, 4)\n",
            "(64,)\n",
            "(64, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3etX-reSALhA",
        "colab_type": "code",
        "outputId": "e696b6de-8e15-4709-e6c7-ab13b8eaf566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "model = load_model('./lstm_sswe_model.h5')\n",
        "print(model.layers[2].get_weights()[0].shape)\n",
        "lstm_sswe_weights = np.zeros(shape = (64,1))\n",
        "x = model.layers[2].get_weights()[0]\n",
        "lstm_sswe_weights = np.sum(x, axis = 1)\n",
        "print(lstm_sswe_weights.shape)\n",
        "lstm_sswe_weights = lstm_sswe_weights.reshape(lstm_sswe_weights.shape[0], 1)\n",
        "print(lstm_sswe_weights.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 4)\n",
            "(64,)\n",
            "(64, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih0zLiP7AlKD",
        "colab_type": "code",
        "outputId": "7c3ede6f-c48e-4fcd-ea9b-ad270e54219b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "glove_sswe_combined = np.concatenate((lstm_sswe_weights, lstm_glove_weights))\n",
        "print(glove_sswe_combined.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D67SwQaGVx-",
        "colab_type": "code",
        "outputId": "c6801403-aad4-469e-c1b5-2579428d749b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(\"Generating GloVe and SSWE combined embedding matrix\")\n",
        "glove_embedding_matrix = create_embedding_matrix(word_index, os.path.join(glove_path, 'glove.840B.300d.txt'))\n",
        "sswe_embedding_matrix = create_embedding_matrix(word_index, os.path.join(sswe_path, 'sswe-r.txt'))\n",
        "padded_training_sequences = pad_sequences(train_sequences, maxlen = MAX_SEQUENCE_LENGTH)\n",
        "labels = to_categorical(np.asarray(labels))\n",
        "padded_validation_sequences = pad_sequences(validation_sequences, maxlen = MAX_SEQUENCE_LENGTH)\n",
        "validation_labels = to_categorical(np.asarray(validation_classes))\n",
        "np.random.shuffle(train_indices)\n",
        "padded_training_sequences = padded_training_sequences[train_indices]\n",
        "labels = labels[train_indices]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating GloVe and SSWE combined embedding matrix\n",
            "2196016 embedding vectors\n",
            "137052 embedding vectors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZIewN82Pukv",
        "colab_type": "text"
      },
      "source": [
        "##### Custom model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvcwakokCRYa",
        "colab_type": "code",
        "outputId": "c33fe81e-195e-4226-f25e-bf7b29ca6696",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"Building custom model\")\n",
        "callback = [ModelCheckpoint('./custom_glovesswecombined_model.h5', verbose = 1, monitor = 'val_loss', save_best_only = True, mode = 'auto'),\n",
        "        EarlyStopping(monitor = 'val_loss', patience = 2),\n",
        "        ComputeMetricsCallback(([np.array(padded_validation_sequences),np.array(padded_validation_sequences)], validation_labels))]\n",
        "custom_glovesswecombined_model = custom_model(glove_embedding_matrix, sswe_embedding_matrix)\n",
        "custom_glovesswecombined_model.fit([np.array(padded_training_sequences),np.array(padded_training_sequences)], np.array(labels), validation_data = ([np.array(padded_validation_sequences),np.array(padded_validation_sequences)], np.array(validation_labels)), epochs = NUM_EPOCHS, batch_size = BATCH_SIZE, shuffle = True, callbacks = callback)\n",
        "custom_glovesswecombined_model = load_model('./custom_glovesswecombined_model.h5')\n",
        "print(\"Generating prediction file\")\n",
        "generate_result_file(model, test_sequences, result_file_name = \"custom_glovesswecombined_predictions.txt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building custom model\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "embedding_5_input (InputLayer)  (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6_input (InputLayer)  (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 64, 300)      4248900     embedding_5_input[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 64, 300)      4248900     embedding_6_input[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "gru_5 (GRU)                     (None, 64, 300)      540900      embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "gru_6 (GRU)                     (None, 64, 300)      540900      embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 64, 32)       28832       gru_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 64, 32)       28832       gru_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 32, 32)       0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, 32, 32)       0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 1024)         0           max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 1024)         0           max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 1024)         0           flatten_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 1024)         0           flatten_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 64)           65600       dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 64)           65600       dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 64)           0           dense_5[0][0]                    \n",
            "                                                                 dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "output_layer (Dense)            (None, 4)            260         add_3[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 9,768,724\n",
            "Trainable params: 9,768,724\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 30160 samples, validate on 2755 samples\n",
            "Epoch 1/10\n",
            "30160/30160 [==============================] - 43s 1ms/step - loss: 1.1780 - acc: 0.4956 - val_loss: 0.7510 - val_acc: 0.8486\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.75104, saving model to ./custom_glovesswecombined_model.h5\n",
            "True Positives per class :  [2338.  142.  125.  150.]\n",
            "False Positives per class :  [ 417. 2613. 2630. 2605.]\n",
            "False Negatives per class :  [0. 0. 0. 0.]\n",
            "Class happy : Precision : 0.052, Recall : 1.000, F1-Score : 0.098\n",
            "Class sad : Precision : 0.045, Recall : 1.000, F1-Score : 0.087\n",
            "Class angry : Precision : 0.054, Recall : 1.000, F1-Score : 0.103\n",
            "Macro Precision : 0.050, Macro Recall : 1.000, Macro F1-Score : 0.096\n",
            "Accuracy : 0.849, Micro Precision : 0.050, Micro Recall : 1.000, Micro F1-Score : 0.096\n",
            "Epoch 2/10\n",
            "30160/30160 [==============================] - 40s 1ms/step - loss: 1.0195 - acc: 0.4956 - val_loss: 0.6478 - val_acc: 0.8486\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.75104 to 0.64777, saving model to ./custom_glovesswecombined_model.h5\n",
            "True Positives per class :  [2338.  142.  125.  150.]\n",
            "False Positives per class :  [ 417. 2613. 2630. 2605.]\n",
            "False Negatives per class :  [0. 0. 0. 0.]\n",
            "Class happy : Precision : 0.052, Recall : 1.000, F1-Score : 0.098\n",
            "Class sad : Precision : 0.045, Recall : 1.000, F1-Score : 0.087\n",
            "Class angry : Precision : 0.054, Recall : 1.000, F1-Score : 0.103\n",
            "Macro Precision : 0.050, Macro Recall : 1.000, Macro F1-Score : 0.096\n",
            "Accuracy : 0.849, Micro Precision : 0.050, Micro Recall : 1.000, Micro F1-Score : 0.096\n",
            "Epoch 3/10\n",
            "30160/30160 [==============================] - 40s 1ms/step - loss: 0.9065 - acc: 0.4956 - val_loss: 0.5700 - val_acc: 0.8486\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.64777 to 0.57002, saving model to ./custom_glovesswecombined_model.h5\n",
            "True Positives per class :  [2338.  142.  125.  150.]\n",
            "False Positives per class :  [ 417. 2613. 2630. 2605.]\n",
            "False Negatives per class :  [0. 0. 0. 0.]\n",
            "Class happy : Precision : 0.052, Recall : 1.000, F1-Score : 0.098\n",
            "Class sad : Precision : 0.045, Recall : 1.000, F1-Score : 0.087\n",
            "Class angry : Precision : 0.054, Recall : 1.000, F1-Score : 0.103\n",
            "Macro Precision : 0.050, Macro Recall : 1.000, Macro F1-Score : 0.096\n",
            "Accuracy : 0.849, Micro Precision : 0.050, Micro Recall : 1.000, Micro F1-Score : 0.096\n",
            "Epoch 4/10\n",
            "30160/30160 [==============================] - 40s 1ms/step - loss: 0.8222 - acc: 0.5346 - val_loss: 0.5363 - val_acc: 0.8192\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.57002 to 0.53626, saving model to ./custom_glovesswecombined_model.h5\n",
            "True Positives per class :  [2009.  115.    0.  133.]\n",
            "False Positives per class :  [ 63. 149.   0. 286.]\n",
            "False Negatives per class :  [329.  27. 125.  17.]\n",
            "Class happy : Precision : 0.436, Recall : 0.810, F1-Score : 0.567\n",
            "Class sad : Precision : nan, Recall : 0.000, F1-Score : 0.000\n",
            "Class angry : Precision : 0.317, Recall : 0.887, F1-Score : 0.467\n",
            "Macro Precision : nan, Macro Recall : 0.566, Macro F1-Score : 0.000\n",
            "Accuracy : 0.819, Micro Precision : 0.363, Micro Recall : 0.595, Micro F1-Score : 0.451\n",
            "Epoch 5/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in float_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "30160/30160 [==============================] - 40s 1ms/step - loss: 0.7588 - acc: 0.7306 - val_loss: 0.5021 - val_acc: 0.8196\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.53626 to 0.50210, saving model to ./custom_glovesswecombined_model.h5\n",
            "True Positives per class :  [2012.  113.    0.  133.]\n",
            "False Positives per class :  [ 62. 143.   0. 292.]\n",
            "False Negatives per class :  [326.  29. 125.  17.]\n",
            "Class happy : Precision : 0.441, Recall : 0.796, F1-Score : 0.568\n",
            "Class sad : Precision : nan, Recall : 0.000, F1-Score : 0.000\n",
            "Class angry : Precision : 0.313, Recall : 0.887, F1-Score : 0.463\n",
            "Macro Precision : nan, Macro Recall : 0.561, Macro F1-Score : 0.000\n",
            "Accuracy : 0.820, Micro Precision : 0.361, Micro Recall : 0.590, Micro F1-Score : 0.448\n",
            "Epoch 6/10\n",
            "30160/30160 [==============================] - 40s 1ms/step - loss: 0.7080 - acc: 0.7349 - val_loss: 0.5026 - val_acc: 0.7989\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.50210\n",
            "True Positives per class :  [1952.  119.    0.  130.]\n",
            "False Positives per class :  [ 53. 182.   0. 319.]\n",
            "False Negatives per class :  [386.  23. 125.  20.]\n",
            "Class happy : Precision : 0.395, Recall : 0.838, F1-Score : 0.537\n",
            "Class sad : Precision : nan, Recall : 0.000, F1-Score : 0.000\n",
            "Class angry : Precision : 0.290, Recall : 0.867, F1-Score : 0.434\n",
            "Macro Precision : nan, Macro Recall : 0.568, Macro F1-Score : 0.000\n",
            "Accuracy : 0.799, Micro Precision : 0.332, Micro Recall : 0.597, Micro F1-Score : 0.427\n",
            "Epoch 7/10\n",
            "30160/30160 [==============================] - 40s 1ms/step - loss: 0.6671 - acc: 0.7419 - val_loss: 0.4650 - val_acc: 0.8305\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.50210 to 0.46503, saving model to ./custom_glovesswecombined_model.h5\n",
            "True Positives per class :  [2044.  112.    0.  132.]\n",
            "False Positives per class :  [ 67. 104.   0. 296.]\n",
            "False Negatives per class :  [294.  30. 125.  18.]\n",
            "Class happy : Precision : 0.519, Recall : 0.789, F1-Score : 0.626\n",
            "Class sad : Precision : nan, Recall : 0.000, F1-Score : 0.000\n",
            "Class angry : Precision : 0.308, Recall : 0.880, F1-Score : 0.457\n",
            "Macro Precision : nan, Macro Recall : 0.556, Macro F1-Score : 0.000\n",
            "Accuracy : 0.830, Micro Precision : 0.379, Micro Recall : 0.585, Micro F1-Score : 0.460\n",
            "Epoch 8/10\n",
            "30160/30160 [==============================] - 40s 1ms/step - loss: 0.6267 - acc: 0.7491 - val_loss: 0.5084 - val_acc: 0.7877\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.46503\n",
            "True Positives per class :  [1921.  111.    0.  138.]\n",
            "False Positives per class :  [ 55. 155.   0. 375.]\n",
            "False Negatives per class :  [417.  31. 125.  12.]\n",
            "Class happy : Precision : 0.417, Recall : 0.782, F1-Score : 0.544\n",
            "Class sad : Precision : nan, Recall : 0.000, F1-Score : 0.000\n",
            "Class angry : Precision : 0.269, Recall : 0.920, F1-Score : 0.416\n",
            "Macro Precision : nan, Macro Recall : 0.567, Macro F1-Score : 0.000\n",
            "Accuracy : 0.788, Micro Precision : 0.320, Micro Recall : 0.597, Micro F1-Score : 0.416\n",
            "Epoch 9/10\n",
            "30160/30160 [==============================] - 40s 1ms/step - loss: 0.6022 - acc: 0.7490 - val_loss: 0.5794 - val_acc: 0.7441\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.46503\n",
            "True Positives per class :  [1794.  114.    0.  142.]\n",
            "False Positives per class :  [ 38. 174.   5. 488.]\n",
            "False Negatives per class :  [544.  28. 125.   8.]\n",
            "Class happy : Precision : 0.396, Recall : 0.803, F1-Score : 0.530\n",
            "Class sad : Precision : 0.000, Recall : 0.000, F1-Score : 0.000\n",
            "Class angry : Precision : 0.225, Recall : 0.947, F1-Score : 0.364\n",
            "Macro Precision : 0.207, Macro Recall : 0.583, Macro F1-Score : 0.306\n",
            "Accuracy : 0.744, Micro Precision : 0.277, Micro Recall : 0.614, Micro F1-Score : 0.382\n",
            "Epoch 10/10\n",
            "30160/30160 [==============================] - 39s 1ms/step - loss: 0.5786 - acc: 0.7519 - val_loss: 0.5158 - val_acc: 0.7804\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.46503\n",
            "True Positives per class :  [1905.  112.    0.  133.]\n",
            "False Positives per class :  [ 55. 191.   0. 359.]\n",
            "False Negatives per class :  [433.  30. 125.  17.]\n",
            "Class happy : Precision : 0.370, Recall : 0.789, F1-Score : 0.503\n",
            "Class sad : Precision : nan, Recall : 0.000, F1-Score : 0.000\n",
            "Class angry : Precision : 0.270, Recall : 0.887, F1-Score : 0.414\n",
            "Macro Precision : nan, Macro Recall : 0.558, Macro F1-Score : 0.000\n",
            "Accuracy : 0.780, Micro Precision : 0.308, Micro Recall : 0.588, Micro F1-Score : 0.404\n",
            "Generating prediction file\n",
            "Model parameters: LSTM Dim : 300, Dropout : 0.2, Batch_size : 200, Learning rate : 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHyzbUJpSHsA",
        "colab_type": "text"
      },
      "source": [
        "### Test phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvLNv1lWSLb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = pd.read_csv(test_path, encoding = 'utf-8', sep = '\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VytpLlaSs2a",
        "colab_type": "code",
        "outputId": "0a2f0c14-49b6-4113-d367-4fb233be9b83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(\"Process training data\")\n",
        "train_indices, train_conversation, labels = load_pre_processed_data(train_path)\n",
        "print(\"Process testing data\")\n",
        "_, test_conversation = load_pre_processed_data(test_path, is_training = False)\n",
        "print(\"Tokenization\")\n",
        "tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(train_conversation)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_conversation)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_conversation)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Process training data\n",
            "Process testing data\n",
            "Tokenization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv3QBh9jTQeU",
        "colab_type": "code",
        "outputId": "36f672cd-4e1e-4956-a6e4-3e2c08799c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "evaluation_model = load_model('./gru_cnn_sswe_model.h5')\n",
        "print(\"Generating prediction file\")\n",
        "generate_result_file(evaluation_model, test_sequences, result_file_name = 'test_gru_cnn_swe_predictions.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating prediction file\n",
            "Model parameters: LSTM Dim : 300, Dropout : 0.2, Batch_size : 200, Learning rate : 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOxPZ201U0IH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, _, test_label = load_pre_processed_data(test_path)\n",
        "_, _, test_predicted = load_pre_processed_data('test_gru_cnn_swe_predictions.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuwb5BexVmGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actual = (to_categorical(np.array(test_label)))\n",
        "predicted = (to_categorical(np.array(test_predicted)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YsTaEz9WIHl",
        "colab_type": "code",
        "outputId": "73e5111a-65d0-4ed2-b30f-96c488565014",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "compute_metrics(predicted, actual)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True Positives per class :  [4214.  200.  187.  234.]\n",
            "False Positives per class :  [174. 180. 171. 149.]\n",
            "False Negatives per class :  [463.  84.  63.  64.]\n",
            "Class happy : Precision : 0.526, Recall : 0.704, F1-Score : 0.602\n",
            "Class sad : Precision : 0.522, Recall : 0.748, F1-Score : 0.615\n",
            "Class angry : Precision : 0.611, Recall : 0.785, F1-Score : 0.687\n",
            "Macro Precision : 0.553, Macro Recall : 0.746, Macro F1-Score : 0.635\n",
            "Accuracy : 0.878, Micro Precision : 0.554, Micro Recall : 0.746, Micro F1-Score : 0.636\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'macro': [0.8776547467779996,\n",
              "  0.5532094240188599,\n",
              "  0.7458201050758362,\n",
              "  0.635235306834371],\n",
              " 'micro': [0.8776547467779996, 0.5539697, 0.7463942, 0.6359447029308016]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    }
  ]
}